{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36fef2bf",
   "metadata": {
    "id": "36fef2bf"
   },
   "source": [
    "# Natural Language Processing in Python\n",
    "\n",
    "## Chapter 7. Intermediate Sequence Modeling for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47036a9",
   "metadata": {
    "id": "c47036a9"
   },
   "source": [
    "### Issue with Vanilla RNNs\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9500c5f",
   "metadata": {
    "id": "a9500c5f"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace\n",
    "from collections import Counter\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import DataVec as dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca184b",
   "metadata": {
    "id": "eaca184b"
   },
   "outputs": [],
   "source": [
    "class UnconditionedSurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, rnn_hidden_size, \n",
    "                 batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_embedding_size (int): The size of the character embeddings\n",
    "            char_vocab_size (int): The number of characters to embed\n",
    "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
    "            batch_first (bool): Informs whether the input tensors will \n",
    "                have batch or the sequence on the 0th dimension\n",
    "            padding_idx (int): The index for the tensor padding; \n",
    "                see torch.nn.Embedding\n",
    "            dropout_p (float): the probability of zeroing activations using\n",
    "                the dropout method.  higher means more likely to zero.\n",
    "        \"\"\"\n",
    "        super(UnconditionedSurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, input_dim)\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
    "        \"\"\"\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "            \n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9579bb",
   "metadata": {
    "id": "aa9579bb"
   },
   "outputs": [],
   "source": [
    "def sample_from_unconditioned_model(model, vectorizer, num_samples=1, sample_size=20, \n",
    "                      temperature=1.0):\n",
    "    \"\"\"Sample a sequence of indices from the model\n",
    "    \n",
    "    Args:\n",
    "        model (SurnameGenerationModel): the trained model\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "        num_samples (int): the number of samples\n",
    "        sample_size (int): the max length of the samples\n",
    "        temperature (float): accentuates or flattens \n",
    "            the distribution. \n",
    "            0.0 < temperature < 1.0 will make it peakier. \n",
    "            temperature > 1.0 will make it more uniform\n",
    "    Returns:\n",
    "        indices (torch.Tensor): the matrix of indices; \n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    h_t = None\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices\n",
    "\n",
    "def decode_samples(sampled_indices, vectorizer):\n",
    "    \"\"\"Transform indices into the string form of a surname\n",
    "    \n",
    "    Args:\n",
    "        sampled_indices (torch.Tensor): the inidces from `sample_from_unconditioned_model`\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "    \"\"\"\n",
    "    decoded_surnames = []\n",
    "    vocab = vectorizer.char_vocab\n",
    "    \n",
    "    for sample_index in range(sampled_indices.shape[0]):\n",
    "        surname = \"\"\n",
    "        for time_step in range(sampled_indices.shape[1]):\n",
    "            sample_item = sampled_indices[sample_index, time_step].item()\n",
    "            if sample_item == vocab.begin_seq_index:\n",
    "                continue\n",
    "            elif sample_item == vocab.end_seq_index:\n",
    "                break\n",
    "            else:\n",
    "                surname += vocab.lookup_index(sample_item)\n",
    "        decoded_surnames.append(surname)\n",
    "    return decoded_surnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e3a1a1",
   "metadata": {
    "id": "58e3a1a1",
    "outputId": "48868778-d392-4031-a15f-a8ad4fa998dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tunconditioned_surname_generation/vectorizer.json\n",
      "\tunconditioned_surname_generation/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    surname_csv=\"surnames_with_splits_sequence.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"unconditioned_surname_generation\",\n",
    "    # Model hyper parameters\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "dv.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "dv.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = dv.SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = dv.SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = UnconditionedSurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d41846",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "93955301c18b4e41a1089cf416c82ff0",
      "6e8914295f7c44d380edf681b04383f3",
      "7f45428462ab4f9ab73141a42ac5e735"
     ]
    },
    "id": "64d41846",
    "outputId": "cf679de5-fcfb-432a-b156-bb6749c34991"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93955301c18b4e41a1089cf416c82ff0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e8914295f7c44d380edf681b04383f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f45428462ab4f9ab73141a42ac5e735",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "train_state = dv.make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = dv.generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = dv.generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # Update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = dv.update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "        \n",
    "        # move model to cpu for sampling\n",
    "        model = model.cpu()\n",
    "        sampled_surnames = decode_samples(\n",
    "            sample_from_unconditioned_model(model, vectorizer, num_samples=2), \n",
    "            vectorizer)\n",
    "        epoch_bar.set_postfix(sample1=sampled_surnames[0], \n",
    "                              sample2=sampled_surnames[1])\n",
    "        # move model back to whichever device it should be on\n",
    "        model = model.to(args.device)\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e649a52",
   "metadata": {
    "id": "3e649a52",
    "outputId": "581a5502-022b-4653-8f38-9e3afa115930"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.6634547424316413;\n",
      "Test Accuracy: 23.645204679841502\n"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = dv.generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    # compute the accuracy\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc \n",
    "\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b919165e",
   "metadata": {
    "id": "b919165e",
    "outputId": "077e7b45-9594-4cc2-ae0d-224e094a282d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------\n",
      "Mefhalseod\n",
      "Pes\n",
      "Allohsoe\n",
      "Strea\n",
      "Nurye\n",
      "Risop\n",
      "ZhisZdeli\n",
      "Zigte\n",
      "Heurcuwensitsy\n",
      "Aegr\n"
     ]
    }
   ],
   "source": [
    "# number of names to generate\n",
    "num_names = 10\n",
    "model = model.cpu()\n",
    "# Generate nationality hidden state\n",
    "sampled_surnames = decode_samples(\n",
    "    sample_from_unconditioned_model(model, vectorizer, num_samples=num_names), \n",
    "    vectorizer)\n",
    "# Show results\n",
    "print (\"-\"*15)\n",
    "for i in range(num_names):\n",
    "    print (sampled_surnames[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8eb1fab",
   "metadata": {
    "id": "e8eb1fab"
   },
   "outputs": [],
   "source": [
    "class ConditionedSurnameGenerationModel(nn.Module):\n",
    "    def __init__(self, char_embedding_size, char_vocab_size, num_nationalities,\n",
    "                 rnn_hidden_size, batch_first=True, padding_idx=0, dropout_p=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            char_embedding_size (int): The size of the character embeddings\n",
    "            char_vocab_size (int): The number of characters to embed\n",
    "            num_nationalities (int): The size of the prediction vector \n",
    "            rnn_hidden_size (int): The size of the RNN's hidden state\n",
    "            batch_first (bool): Informs whether the input tensors will \n",
    "                have batch or the sequence on the 0th dimension\n",
    "            padding_idx (int): The index for the tensor padding; \n",
    "                see torch.nn.Embedding\n",
    "            dropout_p (float): the probability of zeroing activations using\n",
    "                the dropout method.  higher means more likely to zero.\n",
    "        \"\"\"\n",
    "        super(ConditionedSurnameGenerationModel, self).__init__()\n",
    "        \n",
    "        self.char_emb = nn.Embedding(num_embeddings=char_vocab_size,\n",
    "                                     embedding_dim=char_embedding_size,\n",
    "                                     padding_idx=padding_idx)\n",
    "\n",
    "        self.nation_emb = nn.Embedding(num_embeddings=num_nationalities,\n",
    "                                       embedding_dim=rnn_hidden_size)\n",
    "\n",
    "        self.rnn = nn.GRU(input_size=char_embedding_size, \n",
    "                          hidden_size=rnn_hidden_size,\n",
    "                          batch_first=batch_first)\n",
    "        \n",
    "        self.fc = nn.Linear(in_features=rnn_hidden_size, \n",
    "                            out_features=char_vocab_size)\n",
    "        \n",
    "        self._dropout_p = dropout_p\n",
    "\n",
    "    def forward(self, x_in, nationality_index, apply_softmax=False):\n",
    "        \"\"\"The forward pass of the model\n",
    "        \n",
    "        Args:\n",
    "            x_in (torch.Tensor): an input data tensor. \n",
    "                x_in.shape should be (batch, max_seq_size)\n",
    "            nationality_index (torch.Tensor): The index of the nationality for each data point\n",
    "                Used to initialize the hidden state of the RNN\n",
    "            apply_softmax (bool): a flag for the softmax activation\n",
    "                should be false if used with the Cross Entropy losses\n",
    "        Returns:\n",
    "            the resulting tensor. tensor.shape should be (batch, char_vocab_size)\n",
    "        \"\"\"\n",
    "        x_embedded = self.char_emb(x_in)\n",
    "        \n",
    "        # hidden_size: (num_layers * num_directions, batch_size, rnn_hidden_size)\n",
    "        nationality_embedded = self.nation_emb(nationality_index).unsqueeze(0)\n",
    "\n",
    "        y_out, _ = self.rnn(x_embedded, nationality_embedded)\n",
    "\n",
    "        batch_size, seq_size, feat_size = y_out.shape\n",
    "        y_out = y_out.contiguous().view(batch_size * seq_size, feat_size)\n",
    "\n",
    "        y_out = self.fc(F.dropout(y_out, p=self._dropout_p))\n",
    "                         \n",
    "        if apply_softmax:\n",
    "            y_out = F.softmax(y_out, dim=1)\n",
    "\n",
    "        new_feat_size = y_out.shape[-1]\n",
    "        y_out = y_out.view(batch_size, seq_size, new_feat_size)\n",
    "            \n",
    "        return y_out\n",
    "\n",
    "def sample_from_conditioned_model(model, vectorizer, nationalities, sample_size=20, \n",
    "                      temperature=1.0):\n",
    "    \"\"\"Sample a sequence of indices from the model\n",
    "    \n",
    "    Args:\n",
    "        model (SurnameGenerationModel): the trained model\n",
    "        vectorizer (SurnameVectorizer): the corresponding vectorizer\n",
    "        nationalities (list): a list of integers representing nationalities\n",
    "        sample_size (int): the max length of the samples\n",
    "        temperature (float): accentuates or flattens \n",
    "            the distribution. \n",
    "            0.0 < temperature < 1.0 will make it peakier. \n",
    "            temperature > 1.0 will make it more uniform\n",
    "    Returns:\n",
    "        indices (torch.Tensor): the matrix of indices; \n",
    "        shape = (num_samples, sample_size)\n",
    "    \"\"\"\n",
    "    num_samples = len(nationalities)\n",
    "    begin_seq_index = [vectorizer.char_vocab.begin_seq_index \n",
    "                       for _ in range(num_samples)]\n",
    "    begin_seq_index = torch.tensor(begin_seq_index, \n",
    "                                   dtype=torch.int64).unsqueeze(dim=1)\n",
    "    indices = [begin_seq_index]\n",
    "    nationality_indices = torch.tensor(nationalities, dtype=torch.int64).unsqueeze(dim=0)\n",
    "    h_t = model.nation_emb(nationality_indices)\n",
    "    \n",
    "    for time_step in range(sample_size):\n",
    "        x_t = indices[time_step]\n",
    "        x_emb_t = model.char_emb(x_t)\n",
    "        rnn_out_t, h_t = model.rnn(x_emb_t, h_t)\n",
    "        prediction_vector = model.fc(rnn_out_t.squeeze(dim=1))\n",
    "        probability_vector = F.softmax(prediction_vector / temperature, dim=1)\n",
    "        indices.append(torch.multinomial(probability_vector, num_samples=1))\n",
    "    indices = torch.stack(indices).squeeze().permute(1, 0)\n",
    "    return indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbff2ed",
   "metadata": {
    "id": "efbff2ed",
    "outputId": "bdfa308f-fa00-4a60-d7a8-4d371210edda"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded filepaths: \n",
      "\tconditioned_surname_generation/vectorizer.json\n",
      "\tconditioned_surname_generation/model.pth\n",
      "Using CUDA: False\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(\n",
    "    # Data and Path information\n",
    "    surname_csv=\"surnames_with_splits_sequence.csv\",\n",
    "    vectorizer_file=\"vectorizer.json\",\n",
    "    model_state_file=\"model.pth\",\n",
    "    save_dir=\"conditioned_surname_generation\",\n",
    "    # Model hyper parameters\n",
    "    char_embedding_size=32,\n",
    "    rnn_hidden_size=32,\n",
    "    # Training hyper parameters\n",
    "    seed=1337,\n",
    "    learning_rate=0.001,\n",
    "    batch_size=64,\n",
    "    num_epochs=5,\n",
    "    early_stopping_criteria=5,\n",
    "    # Runtime options\n",
    "    catch_keyboard_interrupt=True,\n",
    "    cuda=True,\n",
    "    expand_filepaths_to_save_dir=True,\n",
    "    reload_from_files=False,\n",
    ")\n",
    "\n",
    "if args.expand_filepaths_to_save_dir:\n",
    "    args.vectorizer_file = os.path.join(args.save_dir,\n",
    "                                        args.vectorizer_file)\n",
    "\n",
    "    args.model_state_file = os.path.join(args.save_dir,\n",
    "                                         args.model_state_file)\n",
    "    \n",
    "    print(\"Expanded filepaths: \")\n",
    "    print(\"\\t{}\".format(args.vectorizer_file))\n",
    "    print(\"\\t{}\".format(args.model_state_file))\n",
    "    \n",
    "# Check CUDA\n",
    "if not torch.cuda.is_available():\n",
    "    args.cuda = False\n",
    "\n",
    "args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n",
    "    \n",
    "print(\"Using CUDA: {}\".format(args.cuda))\n",
    "\n",
    "# Set seed for reproducibility\n",
    "dv.set_seed_everywhere(args.seed, args.cuda)\n",
    "\n",
    "# handle dirs\n",
    "dv.handle_dirs(args.save_dir)\n",
    "\n",
    "if args.reload_from_files:\n",
    "    # training from a checkpoint\n",
    "    dataset = dv.SurnameDataset.load_dataset_and_load_vectorizer(args.surname_csv,\n",
    "                                                              args.vectorizer_file)\n",
    "else:\n",
    "    # create dataset and vectorizer\n",
    "    dataset = dv.SurnameDataset.load_dataset_and_make_vectorizer(args.surname_csv)\n",
    "    dataset.save_vectorizer(args.vectorizer_file)\n",
    "\n",
    "vectorizer = dataset.get_vectorizer()\n",
    "\n",
    "model = ConditionedSurnameGenerationModel(char_embedding_size=args.char_embedding_size,\n",
    "                               char_vocab_size=len(vectorizer.char_vocab),\n",
    "                               num_nationalities=len(vectorizer.nationality_vocab),\n",
    "                               rnn_hidden_size=args.rnn_hidden_size,\n",
    "                               padding_idx=vectorizer.char_vocab.mask_index,\n",
    "                               dropout_p=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7089720",
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "24f28d036d3a4097aa2ae14e9af8108e",
      "d3976bde288d4f5ea8f9ba56ba39bd1a",
      "aac388f396354c968f58505c8f39313b"
     ]
    },
    "id": "f7089720",
    "outputId": "36af46e7-7c27-4b18-be3d-4b69af08da81"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f28d036d3a4097aa2ae14e9af8108e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "training routine:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3976bde288d4f5ea8f9ba56ba39bd1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=train:   0%|          | 0/120 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aac388f396354c968f58505c8f39313b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "split=val:   0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mask_index = vectorizer.char_vocab.mask_index\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                           mode='min', factor=0.5,\n",
    "                                           patience=1)\n",
    "train_state = dv.make_train_state(args)\n",
    "\n",
    "epoch_bar = tqdm(desc='training routine', \n",
    "                          total=args.num_epochs,\n",
    "                          position=0)\n",
    "\n",
    "dataset.set_split('train')\n",
    "train_bar = tqdm(desc='split=train',\n",
    "                          total=dataset.get_num_batches(args.batch_size), \n",
    "                          position=1, \n",
    "                          leave=True)\n",
    "dataset.set_split('val')\n",
    "val_bar = tqdm(desc='split=val',\n",
    "                        total=dataset.get_num_batches(args.batch_size), \n",
    "                        position=1, \n",
    "                        leave=True)\n",
    "\n",
    "try:\n",
    "    for epoch_index in range(args.num_epochs):\n",
    "        train_state['epoch_index'] = epoch_index\n",
    "\n",
    "        # Iterate over training dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0, set train mode on\n",
    "        dataset.set_split('train')\n",
    "        batch_generator = dv.generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.0\n",
    "        running_acc = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # the training routine is these 5 steps:\n",
    "\n",
    "            # --------------------------------------    \n",
    "            # step 1. zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # step 2. compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'], \n",
    "                           nationality_index=batch_dict['class_index'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "\n",
    "            # step 4. use loss to produce gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # step 5. use optimizer to take gradient step\n",
    "            optimizer.step()\n",
    "            # -----------------------------------------\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "            # update bar\n",
    "            train_bar.set_postfix(loss=running_loss,\n",
    "                                  acc=running_acc,\n",
    "                                  epoch=epoch_index)\n",
    "            train_bar.update()\n",
    "\n",
    "        train_state['train_loss'].append(running_loss)\n",
    "        train_state['train_acc'].append(running_acc)\n",
    "\n",
    "        # Iterate over val dataset\n",
    "\n",
    "        # setup: batch generator, set loss and acc to 0; set eval mode on\n",
    "        dataset.set_split('val')\n",
    "        batch_generator = dv.generate_batches(dataset, \n",
    "                                           batch_size=args.batch_size, \n",
    "                                           device=args.device)\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        model.eval()\n",
    "\n",
    "        for batch_index, batch_dict in enumerate(batch_generator):\n",
    "            # compute the output\n",
    "            y_pred = model(x_in=batch_dict['x_data'], \n",
    "                           nationality_index=batch_dict['class_index'])\n",
    "\n",
    "            # step 3. compute the loss\n",
    "            loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "            # compute the  running loss and running accuracy\n",
    "            running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "            acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "            running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "            \n",
    "            # Update bar\n",
    "            val_bar.set_postfix(loss=running_loss, acc=running_acc, \n",
    "                            epoch=epoch_index)\n",
    "            val_bar.update()\n",
    "\n",
    "        train_state['val_loss'].append(running_loss)\n",
    "        train_state['val_acc'].append(running_acc)\n",
    "\n",
    "        train_state = dv.update_train_state(args=args, model=model, \n",
    "                                         train_state=train_state)\n",
    "\n",
    "        scheduler.step(train_state['val_loss'][-1])\n",
    "\n",
    "        if train_state['stop_early']:\n",
    "            break\n",
    "            \n",
    "        # move model to cpu for sampling\n",
    "        \n",
    "        nationalities = np.random.choice(np.arange(len(vectorizer.nationality_vocab)), replace=True, size=2)\n",
    "        model = model.cpu()\n",
    "        sampled_surnames = decode_samples(\n",
    "            sample_from_conditioned_model(model, vectorizer, nationalities=nationalities), \n",
    "            vectorizer)\n",
    "        \n",
    "        sample1 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[0]), \n",
    "                                  sampled_surnames[0])\n",
    "        sample2 = \"{}->{}\".format(vectorizer.nationality_vocab.lookup_index(nationalities[1]), \n",
    "                                  sampled_surnames[1])\n",
    "        epoch_bar.set_postfix(sample1=sample1, \n",
    "                              sample2=sample2)\n",
    "        # move model back to whichever device it should be on\n",
    "        model = model.to(args.device)\n",
    "        \n",
    "        train_bar.n = 0\n",
    "        val_bar.n = 0\n",
    "        epoch_bar.update()\n",
    "        \n",
    "except KeyboardInterrupt:\n",
    "    print(\"Exiting loop\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9087ce80",
   "metadata": {
    "id": "9087ce80",
    "outputId": "ff79a280-1dc6-459e-d32f-99b87f8d9150"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'nationality_index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_index, batch_dict \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(batch_generator):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m# compute the output\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     y_pred \u001b[39m=\u001b[39m model(x_in\u001b[39m=\u001b[39;49mbatch_dict[\u001b[39m'\u001b[39;49m\u001b[39mx_data\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39m# compute the loss\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/hritikarasu/Home/PyTorchNLPLessons/Chpt7INTSeqModeling.ipynb#X16sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     loss \u001b[39m=\u001b[39m dv\u001b[39m.\u001b[39msequence_loss(y_pred, batch_dict[\u001b[39m'\u001b[39m\u001b[39my_target\u001b[39m\u001b[39m'\u001b[39m], mask_index)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py:1051\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1047\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1048\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1049\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1052\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1053\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'nationality_index'"
     ]
    }
   ],
   "source": [
    "# compute the loss & accuracy on the test set using the best available model\n",
    "\n",
    "model.load_state_dict(torch.load(train_state['model_filename']))\n",
    "\n",
    "model = model.to(args.device)\n",
    "\n",
    "dataset.set_split('test')\n",
    "batch_generator = dv.generate_batches(dataset, \n",
    "                                   batch_size=args.batch_size, \n",
    "                                   device=args.device)\n",
    "running_acc = 0.\n",
    "model.eval()\n",
    "\n",
    "for batch_index, batch_dict in enumerate(batch_generator):\n",
    "    # compute the output\n",
    "    y_pred = model(x_in=batch_dict['x_data'])\n",
    "\n",
    "    # compute the loss\n",
    "    loss = dv.sequence_loss(y_pred, batch_dict['y_target'], mask_index)\n",
    "\n",
    "    # compute the accuracy\n",
    "    running_loss += (loss.item() - running_loss) / (batch_index + 1)\n",
    "\n",
    "    acc_t = dv.compute_accuracy(y_pred, batch_dict['y_target'], mask_index)\n",
    "    running_acc += (acc_t - running_acc) / (batch_index + 1)\n",
    "\n",
    "train_state['test_loss'] = running_loss \n",
    "train_state['test_acc'] = running_acc \n",
    "\n",
    "print(\"Test loss: {};\".format(train_state['test_loss']))\n",
    "print(\"Test Accuracy: {}\".format(train_state['test_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4634326d",
   "metadata": {
    "id": "4634326d"
   },
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "for index in range(len(vectorizer.nationality_vocab)):\n",
    "    nationality = vectorizer.nationality_vocab.lookup_index(index)\n",
    "    print(\"Sampled for {}: \".format(nationality))\n",
    "    sampled_indices = sample_from_conditioned_model(model, vectorizer,  \n",
    "                                        nationalities=[index] * 3, \n",
    "                                        temperature=0.7)\n",
    "    for sampled_surname in decode_samples(sampled_indices, vectorizer):\n",
    "        print(\"-  \" + sampled_surname)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "07-Intermediate_Sequence_Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ab38ba5a85f824e0e94e6624e7a64cd97870a0e2c6bf30b3938e8e97633cfae6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
